{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    return img\n",
    "\n",
    "def make_heatmap(layer_attn, height_view, width_view):\n",
    "    \n",
    "    matrix = layer_attn.view(height_view, width_view)\n",
    "    matrix = matrix.cpu().numpy()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = \"what does it say in the bottom right corner?\"\n",
    "image_path = \"/home/FRACTAL/snehan.j/Image-Mines/data/post-letter.jpg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "model_path = os.path.expanduser(\"lmms-lab/llama3-llava-next-8b\")\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\"lmms-lab/llama3-llava-next-8b\", None, model_name, attn_implementation=\"eager\")\n",
    "\n",
    "device = \"cuda\"\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "conv_template = \"llava_llama_3\"\n",
    "user_prompt = qs\n",
    "\n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\n {}\".format(user_prompt) + \"\\n Answer briefly in only one sentence\"\n",
    "\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    config, output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "        include_base_only=False,\n",
    "        filter_newline=True)\n",
    "    \n",
    "outputs = tokenizer.batch_decode(output_ids['sequences'], skip_special_tokens=True)[0]\n",
    "\n",
    "sub_crop_height = config[-2]\n",
    "sub_crop_width = config[-1]\n",
    "\n",
    "cum_token_averaged_attn = torch.zeros([1, sub_crop_height * sub_crop_width], device = \"cpu\")\n",
    "img_token_idx = (input_ids == -200).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "for output_attn in output_ids['attentions'][1:]:\n",
    "    stacked_tensors = torch.stack(output_attn).cpu()\n",
    "    stacked_tensors = stacked_tensors[24:32]\n",
    "    mean_values = stacked_tensors.mean(dim=0)\n",
    "    mean_values = mean_values.squeeze()\n",
    "    average_values = mean_values.mean(dim=0)\n",
    "    cum_token_averaged_attn += average_values[img_token_idx + 576: img_token_idx + 576 + sub_crop_height * sub_crop_width]\n",
    "\n",
    "cum_token_averaged_attn = cum_token_averaged_attn/(len(output_ids['attentions'][1:]))\n",
    "attn_matrix = make_heatmap(cum_token_averaged_attn, sub_crop_height, sub_crop_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.4107437e-06, 4.7087669e-06, 1.2536843e-05, ..., 2.1060307e-06,\n",
       "        1.1126200e-06, 8.1857042e-06],\n",
       "       [3.2186508e-06, 1.0867913e-05, 7.1724257e-06, ..., 3.4769375e-06,\n",
       "        3.1391780e-06, 7.7486038e-07],\n",
       "       [1.7881393e-06, 6.4571700e-06, 2.0861626e-06, ..., 9.3181925e-06,\n",
       "        1.0927519e-06, 1.4305115e-06],\n",
       "       ...,\n",
       "       [5.3187210e-05, 3.9021175e-05, 6.1972934e-04, ..., 5.0123531e-04,\n",
       "        7.4605145e-05, 5.2948792e-05],\n",
       "       [6.9340072e-06, 8.5433321e-06, 1.0794401e-04, ..., 3.7650266e-04,\n",
       "        8.3168346e-04, 2.5212765e-05],\n",
       "       [1.4265378e-05, 1.0112922e-05, 3.4173328e-05, ..., 6.9920224e-04,\n",
       "        4.3630600e-05, 5.9386093e-05]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_config = attn_matrix[0].shape\n",
    "image_attn = attn_matrix[0]\n",
    "\n",
    "bounding_boxes, clusters = get_mines(image_attn, image_config[0], image_config[1], min_patch_for_mine=4, max_image_mines = 3, max_search_radius=min(image_config[0], image_config[1])/2, cluster_thresh_frac=0.2, apply_gaussian_blur=1)\n",
    "draw_blocks(img, bounding_boxes, image_config, args.output_path, idx, attn_values[idx][1], upscale_crops=True, delta=15, siglip=False, draw_bool=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
