{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from llava.conversation import conv_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    return img\n",
    "\n",
    "def make_heatmap(layer_attn, height_view, width_view):\n",
    "    \n",
    "    matrix = layer_attn.view(height_view, width_view)\n",
    "    matrix = matrix.cpu().numpy()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input question and image path\n",
    "\n",
    "qs = \"what does it say in the bottom right corner?\"\n",
    "image_path = \"./data/post-letter.jpg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "model_path = os.path.expanduser(\"lmms-lab/llama3-llava-next-8b\")\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\"lmms-lab/llama3-llava-next-8b\", None, model_name, attn_implementation=\"eager\") # loading model and tokenizer\n",
    "\n",
    "device = \"cuda\"\n",
    "image_tensor = process_images([image], image_processor, model.config)\n",
    "image_tensor = [_image.to(dtype=torch.float16, device=device) for _image in image_tensor]\n",
    "\n",
    "conv_template = \"llava_llama_3\"\n",
    "user_prompt = qs\n",
    "\n",
    "question = DEFAULT_IMAGE_TOKEN + \"\\n {}\".format(user_prompt) + \"\\n Answer briefly in only one sentence\"\n",
    "\n",
    "conv = copy.deepcopy(conv_templates[conv_template])\n",
    "conv.append_message(conv.roles[0], question)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt_question = conv.get_prompt()\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt_question, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\").unsqueeze(0).to(device)\n",
    "image_sizes = [image.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    # generating output_ids\n",
    "    config, output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "        include_base_only=False,\n",
    "        filter_newline=True)\n",
    "    \n",
    "outputs = tokenizer.batch_decode(output_ids['sequences'], skip_special_tokens=True)[0]\n",
    "\n",
    "sub_crop_height = config[-2]\n",
    "sub_crop_width = config[-1]\n",
    "\n",
    "cum_token_averaged_attn = torch.zeros([1, sub_crop_height * sub_crop_width], device = \"cpu\")\n",
    "img_token_idx = (input_ids == -200).nonzero(as_tuple=True)[1].item()\n",
    "\n",
    "for output_attn in output_ids['attentions'][1:]:\n",
    "    stacked_tensors = torch.stack(output_attn).cpu()\n",
    "    stacked_tensors = stacked_tensors[24:32]\n",
    "    mean_values = stacked_tensors.mean(dim=0)\n",
    "    mean_values = mean_values.squeeze()\n",
    "    average_values = mean_values.mean(dim=0)\n",
    "    cum_token_averaged_attn += average_values[img_token_idx + 576: img_token_idx + 576 + sub_crop_height * sub_crop_width]\n",
    "\n",
    "cum_token_averaged_attn = cum_token_averaged_attn/(len(output_ids['attentions'][1:]))\n",
    "attn_matrix = make_heatmap(cum_token_averaged_attn, sub_crop_height, sub_crop_width) # output attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "list = []\n",
    "list.append((attn_matrix))\n",
    "\n",
    "with open('./data/output/attn.pkl', 'wb') as f:\n",
    "    pickle.dump(list, f) # dumping the attention matrix into a pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
